---
title: "ESM 244 Lab 2"
author: "Danielle Hoekstra"
date: "2023-01-19"
output: html_document
---

```{r setup, warning = FALSE, echo = TRUE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(palmerpenguins)
library(AICcmodavg)
library(equatiomatic)
```

# Predicting penguin mass
```{r}

penguins_clean <- penguins %>% 
  drop_na() %>% 
  rename(mass = body_mass_g,
         bill_l = bill_length_mm,
         bill_d = bill_depth_mm,
         flip_l = flipper_length_mm)

mdl1 <- lm(mass ~ bill_l + bill_d + flip_l + species + sex + island, 
           data = penguins_clean)

##to see how good your model is use summary(mdl1), get the coefficients 
```

```{r}
f1 <- mass ~ bill_l + bill_d + flip_l + species + sex + island #R will create this as a formula, can plug in f1 as a substitution and clean it up

mdl1 = lm(f1, data = penguins_clean)

f2 <- mass ~ bill_l + bill_d + flip_l + species + sex

mdl2 = lm(f2, data = penguins_clean)

AIC(mdl1, mdl2) # how many degrees of freedom and then AIC, AIC is lower in mdl2 but barely, still predicts as well

f3 <- mass ~ bill_d + flip_l + species + sex

mdl3 <- lm(f3, data = penguins_clean)

AIC(mdl1, mdl2, mdl3) # predictive power has gotten worse in mdl3 because the AIC is higher

BIC(mdl1, mdl2, mdl3) # mdl3 is actually closer to model 2, BIC rewards parsimony more strongly

AICcmodavg::AICc(mdl1) # the AICcmodavg:: reminder that the next function is coming from that package

aictab(list(mdl1, mdl2, mdl3))
bictab(list(mdl1, mdl2, mdl3)) 

#this is one way to create a linear model and decide if you should include a variable or which ones you should drop out
```

# Compare models using k-fold validation
```{r}
folds <- 10 #take our dataset and break it into 10 chunks, set aside one of those chunks and then test it against what we have set aside
fold_vec <- rep(1:folds, length.out = nrow(penguins_clean))

set.seed(42) #to shuffle
penguins_fold <- penguins_clean %>% 
  mutate(group = sample(fold_vec, size = n(), replace = FALSE))

table(penguins_fold$group) #how many observations are in each group, check for even

test_df <- penguins_fold %>% 
  filter(group ==1)

train_df <- penguins_fold %>% 
  filter(group != 1)
```

```{r}
## calc_mean <- function(x) {m <- sum(x) / length(x)} example of what we are doing

calc_rmse <- function(x, y) {
  rmse <- (x - y)^2 %>% 
    mean() %>% 
    sqrt()
  return(rmse)
}
```

```{r}
#use the training data frame, not the entire penguins clean
training_mdl1 <- lm(f1, data = train_df) #based on this smaller set, heres how we would predict the mass using the variables in f1
training_mdl2 <- lm(f2, data = train_df)

training_mdl3 <- lm(f3, data = train_df)

#see how well these predict models in the test value frame
predict_test <- test_df %>% mutate(model1 = predict(training_mdl1, test_df), 
                                   model2 = predict(training_mdl2, test_df), 
                                   model3 = predict(training_mdl3, test_df)) 
view(predict_test)
#predict the masses from training body mass how well they match the test data frame
#error will be the difference between these two values which is the rmse value

rmse_predict_test <-predict_test %>% 
  summarize(rmse_mdl1 = calc_rmse(model1, mass),
            rmse_mdl2 = calc_rmse(model2, mass), 
            rmse_mdl3 = calc_rmse(model3, mass)) 

#take model 1 column, and subtract the known mass and square all of the differences (so the larger differences are more highly penalized, rmse 'root means square error') or whichever model you are using

view(rmse_predict_test)
# model 2 has the lowest value, therefore would be the closest fit

#therefore how would we do this for all 10 of our folds, could be tedious

```

# Lets iterate!
```{r}
rmse_df <- data.frame()

#assign it to a variable i, for every list in i it will go through everything in the {} for every list
for(i in 1:folds) {
  ### i <- 1
  kfold_test_df <- penguins_fold %>% 
    filter(group == i)
  kfold_train_df <- penguins_fold %>% 
    filter(group != i)
  
  kfold_mdl1 <- lm(f1, data = kfold_train_df)
  kfold_mdl2 <- lm(f2, data = kfold_train_df)
  kfold_mdl3 <- lm(f3, data = kfold_train_df)
  
  kfold_pred_df <- kfold_test_df %>% 
    mutate(mdl1 = predict(kfold_mdl1, .), 
           mdl2 = predict(kfold_mdl2, .), 
           mdl3 = predict(kfold_mdl3, .))
  
  kfold_rmse_df <- kfold_pred_df %>% 
    summarise(rmse_mdl1 = calc_rmse(mdl1, mass), 
              rmse_mdl2 = calc_rmse(mdl2, mass),
              rmse_mdl3 = calc_rmse(mdl3, mass),
              test_gp = i)
  
  rmse_df <- bind_rows(rmse_df, kfold_rmse_df)
}

#the period means use the same data frame as called out above

rmse_df %>% 
  summarize(mean_rmse_mdl1 = mean(rmse_mdl1),
            mean_rmse_mdl2 = mean(rmse_mdl2),
            mean_rmse_mdl3 = mean(rmse_mdl3))
```
# Finalize the model

```{r}
final_mdl <- lm(f2, data = penguins_clean)
```

Our final model:
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE)`

And with coefficients:
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE, use_coefs = TRUE)`












